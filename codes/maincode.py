# -*- coding: utf-8 -*-
"""튜토리얼 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MYCeyxGMUIFObl6H_Ws7JdUPmP07z2B-
"""

import os
import glob
import json
import pprint

import numpy as np
from scipy.sparse import csr_matrix

from lightgbm import LGBMClassifier

from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier

from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import RFE

from sklearn.feature_extraction.text import TfidfVectorizer
import hashlib
from xgboost import XGBClassifier
import csv

# 로컬에서 작동시키는 것을 전제로 만들었습니다.
# from google.colab import drive
# drive.mount('/content/drive')

def read_label_csv(path):
    label_table = dict()
    with open(path, "r") as f:
        for line in f.readlines()[1:]:
            fname, label = line.strip().split(",")
            label_table[fname] = int(label)
    return label_table


def read_json(path):
    with open(path, "r") as f:
        return json.load(f)


def load_model(**kwargs):
    if kwargs["model"] == "rf":
        return RandomForestClassifier(random_state=kwargs["random_state"], n_jobs=4)
    elif kwargs["model"] == "dt":
        return DecisionTreeClassifier(random_state=kwargs["random_state"])
    elif kwargs["model"] == "lgb":
        return LGBMClassifier(random_state=kwargs["random_state"])
    elif kwargs["model"] == "svm":
        return SVC(random_state=kwargs["random_state"])
    elif kwargs["model"] == "lr":
        return LogisticRegression(random_state=kwargs["random_state"], n_jobs=-1)
    elif kwargs["model"] == "knn":
        return KNeighborsClassifier(n_jobs=-1)
    elif kwargs["model"] == "adaboost":
        return AdaBoostClassifier(random_state=kwargs["random_state"])
    elif kwargs["model"] == "mlp":
        return MLPClassifier(random_state=kwargs["random_state"])
    elif kwargs["model"] == "xg":
        return XGBClassifier(random_state=kwargs["random_state"])
    else:
        print("Unsupported Algorithm")
        return None


def train(X_train, y_train, model):
    """
        머신러닝 모델을 선택하여 학습을 진행하는 함수

        :param X_train: 학습할 2차원 리스트 특징벡터
        :param y_train: 학습할 1차원 리스트 레이블 벡터
        :param model: 문자열, 선택할 머신러닝 알고리즘
        :return: 학습된 머신러닝 모델 객체
    """
    clf = load_model(model=model, random_state=SEED)
    clf.fit(X_train, y_train)
    return clf


def evaluate(X_test, y_test, model):
    """
        학습된 머신러닝 모델로 검증 데이터를 검증하는 함수
        :param X_test: 검증할 2차원 리스트 특징 벡터
        :param y_test: 검증할 1차원 리스트 레이블 벡터
        :param model: 학습된 머신러닝 모델 객체
    """
    predict = model.predict(X_test)
    print("정확도", model.score(X_test, y_test))


"""## 특징 벡터 생성 예시
- PEMINER 정보는 모두 수치형 데이터이므로 특별히 가공을 하지 않고 사용 가능
- EMBER, PESTUDIO 정보는 가공해서 사용해야 할 특징들이 있음 (e.g. imports, exports 등의 문자열 정보를 가지는 데이터)
- 수치형 데이터가 아닌 데이터(범주형 데이터)를 어떻게 가공할 지가 관건 >> 인코딩 (e.g. 원핫인코딩, 레이블인코딩 등)
"""


class PeminerParser:
    def __init__(self, path):
        self.report = read_json(path)
        self.vector = []

    def process_report(self):
        '''
            전체 데이터 사용        
        '''
        self.vector = [value for _, value in sorted(self.report.items(), key=lambda x: x[0])]
        return self.vector


class EmberParser:
    '''
        예제에서 사용하지 않은 특징도 사용하여 벡터화 할 것을 권장
    '''

    def __init__(self, path):
        self.report = read_json(path)
        self.strlist = []
        self.hasher = hashlib.new("md5")

    def get_histogram_info(self):
        histogram = np.array(self.report["histogram"])
        total = histogram.sum()
        vector = histogram / total
        return vector.tolist()

    def get_string_info(self):
        strings = self.report["strings"]

        hist_divisor = float(strings['printables']) if strings['printables'] > 0 else 1.0
        vector = [
            strings['numstrings'],
            strings['avlength'],
            strings['printables'],
            strings['entropy'],
            strings['paths'],
            strings['urls'],
            strings['registry'],
            strings['MZ']
        ]
        vector += (np.asarray(strings['printabledist']) / hist_divisor).tolist()
        return vector

    def get_general_file_info(self):
        general = self.report["general"]
        vector = [
            general['size'], general['vsize'], general['has_debug'], general['exports'], general['imports'],
            general['has_relocations'], general['has_resources'], general['has_signature'], general['has_tls'],
            general['symbols']
        ]
        return vector

    def get_section_info(self):
        # 나머지는 비슷하거나 영향이 미미하므로 "characteristics"만 가져옴
        section = self.report["section"]["sections"]
        name_size = 20
        props_size = 45
        name_vec = [0 for i in range(name_size)]
        props_vec = [0 for i in range(props_size)]

        size_sum, ent_sum, vs_sum = 0, 0, 0
        if len(section) == 0:
            print('EMP')
        for i in section:
            if not i["name"] == "":
                self.hasher.update(i["name"].encode("UTF-8"))  # String
                name_vec[int(self.hasher.hexdigest(), 16) % name_size] += 1
                for h in i["props"]:
                    self.hasher.update(h.encode("UTF-8"))  # String
                    props_vec[int(self.hasher.hexdigest(), 16) % props_size] += 1

            size_sum += i["size"]
            ent_sum += i["entropy"]
            vs_sum += i["vsize"]
        size_sum = size_sum / len(section) if size_sum > 0 else 0
        ent_sum = ent_sum / len(section) if ent_sum > 0 else 0
        vs_sum = vs_sum / len(section) if vs_sum > 0 else 0
        vector = [size_sum, ent_sum, vs_sum]
        vector.extend(name_vec)
        vector.extend(props_vec)

        return vector

    def get_import_info(self):
        imports = self.report["imports"]
        imp_size = 20
        imp_vec = [0 for i in range(imp_size)]
        for i in imports:
            for j in i:
                self.hasher.update(j.encode("UTF-8"))  # String
                imp_vec[int(self.hasher.hexdigest(), 16) % imp_size] += 1

        return imp_vec


    def get_export_info(self):
        exports = self.report["exports"]
        self.strlist.extend(exports)

    def process_report(self):
        vector = []
        vector += self.get_general_file_info()
        vector += self.get_histogram_info()
        vector += self.get_string_info()

        # Add features
        vector += self.get_section_info()
        self.get_import_info()
        vector.extend(self.strlist)

        return vector


class PestudioParser:
    '''
        사용할 특징을 선택하여 벡터화 할 것을 권장
    '''

    def __init__(self, path):
        self.report = read_json(path)
        self.hasher = hashlib.new("md5")

    def get_overview_info(self):
        overview = self.report["image"]["overview"]
        self.hasher.update(overview["size"].encode("UTF-8"))
        vector = [int(self.hasher.hexdigest(), 16) % 10, float(overview["entropy"])]
        return vector

    def get_library_info(self):
        lib_vec = [0 for j in range(20)]
        try:
            library = self.report["image"]["libraries"]["library"]
        except KeyError:
            return lib_vec
        if type(library) == dict:
            self.hasher.update(library["@name"].encode("UTF-8"))  # String
            lib_vec[int(self.hasher.hexdigest(), 16) % 20] += 1
        else:
            for i in library:
                self.hasher.update(str(i["@name"]).encode("UTF-8"))  # String
                lib_vec[int(self.hasher.hexdigest(), 16) % 20] += 1


        return lib_vec

    def process_report(self):
        vector = []
        vector += self.get_overview_info()
        vector += self.get_library_info()

        return vector


"""## 학습데이터 구성
- 특징 벡터 구성은 2차원이 되어야함 e.g.  [vector_1, vector_2, ..., vector_n]

- 각 벡터는 1차원 리스트, 벡터 크기는 모두 같아야함
"""

# 데이터의 특징 벡터 모음(2차원 리스트) : X
# 데이터의 레이블 모음(1차원 리스트) : y
# X, y = [], []
#
# for fname in ["000c4ae5e00a1d4de991a9decf9ecbac59ed5582f5972f05b48bc1a1fe57338a",
#               "00ed7bc707559e6e63818b2bba0ac6b338ba17d95aea6f0838cbdc40cb9acd94"]:
#     feature_vector = []
#     label = label_table[fname]
#     for data in ["PEMINER", "EMBER"]:
#         path = f"/content/drive/My Drive/데이터/{data}/학습데이터/{fname}.json"
#
#         if data == "PEMINER":
#             feature_vector += PeminerParser(path).process_report()
#         else:
#             feature_vector += EmberParser(path).process_report()
#     X.append(feature_vector)
#     y.append(label)
# print(X)
# print(y)
# np.asarray(X).shape, np.asarray(y).shape


# %%
"""
## 데이터 벡터 구성
- 특징 벡터 구성은 2차원이 되어야함 e.g.  [vector_1, vector_2, ..., vector_n]
- 각 벡터는 1차원 리스트, 벡터 크기는 모두 같아야함
- process1 함수를 사용하시면 데이터의 벡터 리스트와 라벨 리스트를 쉽게 얻는다
- 원래의 튜토리얼 코드에는 없었음. 결합시킴.
"""


# 원본
def process2(path_l, peminer, ember, pestudio):
    V, w, S = [], [], []
    x = 0
    label_table = read_label_csv(f"{path_l}")
    a = os.listdir(f"{peminer}")
    b = os.listdir(f"{ember}")
    c = os.listdir(f"{pestudio}")
    a.extend(b)
    a.extend(c)
    all_files = set(a)
    for fname in list(all_files):
        feature_vector = []
        tmpvec = []
        strlist = ""
        lname = fname.split('.')[0]
        label = label_table[lname]
        for data in [peminer, ember, pestudio]:
            path = f"{data}/{fname}"
            try:
                if data == peminer:
                    feature_vector += PeminerParser(path).process_report()
                elif data == ember:
                    tmpvec = EmberParser(path).process_report()
                    # feature_vector.extend(tmpvec)
                    feature_vector += tmpvec
                else:
                    feature_vector += PestudioParser(path).process_report()
            except FileNotFoundError:
                if data == peminer:
                    feature_vector += [0 for i in range(188)]
                elif data == ember:
                    feature_vector += [0 for i in range(438)]
                else:
                    feature_vector += [0 for i in range(22)]
            except TypeError:
                print(path)
            except ValueError:
                print('Error occured : ' + fname)

        V.append(feature_vector)
        w.append(label)
        x += 1
        print("path_l count : " , x)
    return V, w


def test_process(peminer, ember, pestudio):
    V = []
    x = 0
    a = os.listdir(f"{peminer}")
    b = os.listdir(f"{ember}")
    c = os.listdir(f"{pestudio}")
    a.extend(b)
    a.extend(c)
    all_files = set(a)
    for fname in list(all_files):
        feature_vector = []
        lname = fname.split('.')[0]
        for data in [peminer, ember, pestudio]:
            path = f"{data}/{fname}"
            try:
                if data == peminer:
                    feature_vector += PeminerParser(path).process_report()
                elif data == ember:
                    tmpvec = EmberParser(path).process_report()
                    # feature_vector.extend(tmpvec)
                    feature_vector += tmpvec
                else:
                    feature_vector += PestudioParser(path).process_report()
            except FileNotFoundError:
                if data == peminer:
                    feature_vector += [0 for i in range(188)]
                elif data == ember:
                    feature_vector += [0 for i in range(438)]
                else:
                    feature_vector += [0 for i in range(22)]
            except TypeError:
                print(path)
            except ValueError:
                print('Error occured : ' + fname)
        V.append(feature_vector)
        x += 1
        print("path_l count : " , x)
    return V
# %%
"""## 앙상블 예제"""


# %%
def ensemble_result(X, y, models):
    """
        학습된 모델들의 결과를 앙상블하는 함수
        :param X: 검증할 2차원 리스트 특징 벡터
        :param y: 검증할 1차원 리스트 레이블 벡터
        :param models: 1개 이상의 학습된 머신러닝 모델 객체를 가지는 1차원 리스트
    """

    # Soft Voting
    # https://devkor.tistory.com/entry/Soft-Voting-%EA%B3%BC-Hard-Voting
    predicts = []
    for i in range(len(X)):
        probs = []
        for model in models:
            prob = model.predict_proba(X)[i][1]
            probs.append(prob)
        predict = 1 if np.mean(probs) >= 0.5 else 0
        predicts.append(predict)

    print("정확도", accuracy_score(y, predicts))


# %%
"""## 특징 선택 예제 (RFE 알고리즘 사용)"""


# %%
def select_feature(X, y, model):
    '''
        주어진 특징 벡터에서 특정 알고리즘 기반 특징 선택
        
        본 예제에서는 RFE 알고리즘 사용
        https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE.fit_transform
        
        :param X: 검증할 2차원 리스트 특징 벡터
        :param y: 검증할 1차원 리스트 레이블 벡터
        :param model: 문자열, 특징 선택에 사용할 머신러닝 알고리즘
    '''

    model = load_model(model=model, random_state=SEED)
    rfe = RFE(estimator=model)
    return rfe.fit_transform(X, y)


def save_csv(filename, predict):
    f = open('./predict.csv', 'w', encoding='utf-8', newline='')
    wr = csv.writer(f)
    wr.writerow(['file', 'predict'])

    for idx in range(len(filename)):
        wr.writerow([filename[idx][:-5], predict[idx]])

    f.close()


def save_ensemble_result(X, models, path=[]):

    # Soft Voting
    # https://devkor.tistory.com/entry/Soft-Voting-%EA%B3%BC-Hard-Voting
    predicts = []
    count = 0
    for model in models:
        prob = [result for _, result in model.predict_proba(X)]
        predicts.append(prob)

    predict = np.mean(predicts, axis=0)
    predict = [1 if x >= 0.5 else 0 for x in predict]
    save_csv(path, predict)


if __name__ == '__main__':
    SEED = 41
    datapath = "../../../../Data/"

    trainlabel = datapath + "학습데이터_정답.csv"
    checklable = datapath + "검증데이터_정답.csv"

    # Path for test data
    peminer = datapath + "PEMINER/테스트데이터"
    ember = datapath + "EMBER/테스트데이터"
    pestudio = datapath + "PESTUDIO/테스트데이터"

    # Path for check data
    check_peminer = datapath + "PEMINER/검증데이터"
    check_ember = datapath + "EMBER/검증데이터"
    check_pestudio = datapath + "PESTUDIO/검증데이터"

    # Path for train data
    train_peminer = datapath + "PEMINER/학습데이터"
    train_ember = datapath + "EMBER/학습데이터"
    train_pestudio = datapath + "PESTUDIO/학습데이터"

    X, y = process2(trainlabel, train_peminer, train_ember, train_pestudio)
    testV = test_process(peminer, ember, pestudio)

    aeee = np.asarray(X)
    csr = csr_matrix(aeee)
    models = []
    for model in ["xg"]:
        clf = train(csr, y, model)
        models.append(clf)
    save_ensemble_result(testV, models, os.listdir(peminer))
    # 검증
    # 실제 검증 시에는 제공한 검증데이터를 검증에 사용해야 함
    CKX, CKY = process2(checklable, check_peminer, check_ember, check_pestudio)
    for model in models:
        evaluate(CKX, CKY, model)
    ensemble_result(CKX, CKY, models)

    # selected_X = select_feature(X, y, "rf")
    # new_model = train(selected_X, y, "rf")
